#!/usr/bin/env python3
"""
CSV Log Analysis Tool

This script analyzes the CSV logs generated by the object detection system.
It provides various analytics and insights about the processed images and videos.

Usage:
    python analyze_csv_logs.py [csv_file_path]
    
If no CSV file is specified, it will analyze the most recent log file.
"""

import pandas as pd
import os
import glob
from collections import Counter
import argparse
from datetime import datetime

def find_latest_csv_log():
    """Find the most recent CSV log file"""
    csv_pattern = "./csv_logs/object_detection_log_*.csv"
    csv_files = glob.glob(csv_pattern)
    
    if not csv_files:
        print("No CSV log files found in ./csv_logs/")
        return None
    
    # Sort by modification time and get the most recent
    latest_file = max(csv_files, key=os.path.getmtime)
    return latest_file

def analyze_csv_log(csv_filepath):
    """Analyze CSV log file and generate insights"""
    
    if not os.path.exists(csv_filepath):
        print(f"CSV file not found: {csv_filepath}")
        return
    
    print(f"Analyzing CSV log: {csv_filepath}")
    print("=" * 60)
    
    # Read CSV file
    df = pd.read_csv(csv_filepath)
    
    # Basic statistics
    print("\nðŸ“Š BASIC STATISTICS")
    print("-" * 30)
    print(f"Total entries: {len(df)}")
    print(f"Unique files processed: {df['file_path'].nunique()}")
    print(f"Content types: {', '.join(df['content_type'].unique())}")
    
    # Content type breakdown
    content_counts = df['content_type'].value_counts()
    print(f"\nContent breakdown:")
    for content_type, count in content_counts.items():
        print(f"  - {content_type}: {count} entries")
    
    # Processing time analysis
    if 'processing_time_seconds' in df.columns:
        avg_time = df['processing_time_seconds'].astype(float).mean()
        total_time = df['processing_time_seconds'].astype(float).sum()
        print(f"\nProcessing time:")
        print(f"  - Average per item: {avg_time:.2f} seconds")
        print(f"  - Total processing time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)")
    
    # Object detection analysis
    print("\nðŸ” OBJECT DETECTION ANALYSIS")
    print("-" * 35)
    
    # Filter out NO_OBJECTS_DETECTED entries
    objects_df = df[df['object_name'] != 'NO_OBJECTS_DETECTED']
    no_objects_count = len(df[df['object_name'] == 'NO_OBJECTS_DETECTED'])
    
    print(f"Items with detected objects: {len(objects_df)}")
    print(f"Items with no objects detected: {no_objects_count}")
    
    if len(objects_df) > 0:
        # Most common objects
        object_counts = Counter(objects_df['object_name'].tolist())
        print(f"\nðŸ† TOP 10 DETECTED OBJECTS:")
        for obj, count in object_counts.most_common(10):
            print(f"  {count:3d}x {obj}")
        
        # Objects per file
        objects_per_file = objects_df.groupby('file_path')['object_name'].count()
        print(f"\nAverage objects per file: {objects_per_file.mean():.1f}")
        print(f"Max objects in single file: {objects_per_file.max()}")
        
        # Most object-rich files
        print(f"\nðŸ“ FILES WITH MOST OBJECTS:")
        top_files = objects_per_file.nlargest(5)
        for filepath, count in top_files.items():
            filename = os.path.basename(filepath)
            print(f"  {count:2d} objects: {filename}")
    
    # Video-specific analysis
    video_df = df[df['content_type'] == 'video_frame']
    if len(video_df) > 0:
        print("\nðŸŽ¥ VIDEO ANALYSIS")
        print("-" * 20)
        print(f"Total video frames processed: {len(video_df)}")
        print(f"Unique videos: {video_df['file_path'].nunique()}")
        
        # Timeline analysis for videos
        if 'timestamp_seconds' in video_df.columns:
            video_df['timestamp_seconds'] = pd.to_numeric(video_df['timestamp_seconds'], errors='coerce')
            
            for video_path in video_df['file_path'].unique()[:3]:  # Show first 3 videos
                video_frames = video_df[video_df['file_path'] == video_path]
                filename = os.path.basename(video_path)
                max_time = video_frames['timestamp_seconds'].max()
                frame_count = len(video_frames)
                
                print(f"\n  ðŸ“½ï¸  {filename}:")
                print(f"    Duration covered: {max_time:.1f} seconds")
                print(f"    Frames analyzed: {frame_count}")
                
                # Objects detected in this video
                video_objects = video_frames[video_frames['object_name'] != 'NO_OBJECTS_DETECTED']
                if len(video_objects) > 0:
                    unique_objects = video_objects['object_name'].unique()
                    print(f"    Objects detected: {', '.join(unique_objects[:5])}")
                    if len(unique_objects) > 5:
                        print(f"    ... and {len(unique_objects) - 5} more")
    
    # Image-specific analysis
    image_df = df[df['content_type'] == 'image']
    if len(image_df) > 0:
        print("\nðŸ“¸ IMAGE ANALYSIS")
        print("-" * 18)
        print(f"Total images processed: {len(image_df)}")
        
        # EXIF data analysis
        if 'make' in image_df.columns:
            cameras = image_df[image_df['make'] != '']['make'].value_counts()
            if len(cameras) > 0:
                print(f"\nCamera makes:")
                for make, count in cameras.head().items():
                    print(f"  {count}x {make}")
        
        # GPS data
        if 'gps_coordinates' in image_df.columns:
            gps_count = len(image_df[image_df['gps_coordinates'] != ''])
            print(f"\nImages with GPS data: {gps_count}")
    
    # Session timeline
    if 'session_timestamp' in df.columns:
        print("\nâ° SESSION TIMELINE")
        print("-" * 20)
        df['session_timestamp'] = pd.to_datetime(df['session_timestamp'])
        start_time = df['session_timestamp'].min()
        end_time = df['session_timestamp'].max()
        duration = end_time - start_time
        
        print(f"Session started: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Session ended: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Total session duration: {duration}")
    
    print("\n" + "=" * 60)
    print("Analysis complete! ðŸŽ‰")
    
    return df

def export_summary_report(df, csv_filepath):
    """Export a summary report to a text file"""
    
    base_name = os.path.splitext(csv_filepath)[0]
    report_filepath = f"{base_name}_summary.txt"
    
    with open(report_filepath, 'w', encoding='utf-8') as f:
        f.write("OBJECT DETECTION LOG ANALYSIS REPORT\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Source CSV: {csv_filepath}\n\n")
        
        # Basic stats
        f.write(f"Total entries: {len(df)}\n")
        f.write(f"Unique files: {df['file_path'].nunique()}\n")
        f.write(f"Content types: {', '.join(df['content_type'].unique())}\n\n")
        
        # Object summary
        objects_df = df[df['object_name'] != 'NO_OBJECTS_DETECTED']
        if len(objects_df) > 0:
            object_counts = Counter(objects_df['object_name'].tolist())
            f.write("TOP DETECTED OBJECTS:\n")
            for obj, count in object_counts.most_common(20):
                f.write(f"  {count:3d}x {obj}\n")
        
        f.write(f"\nReport saved to: {report_filepath}\n")
    
    print(f"ðŸ“„ Summary report saved to: {report_filepath}")

def main():
    parser = argparse.ArgumentParser(description='Analyze object detection CSV logs')
    parser.add_argument('csv_file', nargs='?', help='Path to CSV log file')
    parser.add_argument('--export-report', '-r', action='store_true', 
                        help='Export summary report to text file')
    
    args = parser.parse_args()
    
    # Determine CSV file to analyze
    if args.csv_file:
        csv_filepath = args.csv_file
    else:
        csv_filepath = find_latest_csv_log()
        if not csv_filepath:
            return
    
    # Analyze the CSV file
    df = analyze_csv_log(csv_filepath)
    
    # Export report if requested
    if args.export_report and df is not None:
        export_summary_report(df, csv_filepath)

if __name__ == "__main__":
    main()